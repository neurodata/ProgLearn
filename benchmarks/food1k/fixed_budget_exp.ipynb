{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.transformers import (\n",
    "    NeuralClassificationTransformer,\n",
    "    TreeClassificationTransformer,\n",
    ")\n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.backend import clear_session \n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATADIR = '/Users/jayantadey/Downloads/LargeFineFoodAI/train'\n",
    "VAL_DATADIR = '/Users/jayantadey/Downloads/LargeFineFoodAI/Val'\n",
    "\n",
    "CATEGORIES = list(range(20))\n",
    "SAMPLE_PER_CLASS = 60\n",
    "NUM_CLASS_PER_TASK = 20\n",
    "IMG_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "test_X = []\n",
    "test_y = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(TRAIN_DATADIR, str(category))\n",
    "    \n",
    "    images = os.listdir(path)\n",
    "    total_images = len(images)\n",
    "    train_indx = sample(range(total_images), SAMPLE_PER_CLASS)\n",
    "    test_indx = np.delete(range(total_images), train_indx)\n",
    "    for ii in train_indx:\n",
    "        image_data = cv2.imread(\n",
    "                os.path.join(path, images[ii])\n",
    "            )\n",
    "        resized_image = cv2.resize(\n",
    "            image_data, \n",
    "            (IMG_SIZE, IMG_SIZE)\n",
    "        )\n",
    "        train_X.append(\n",
    "            resized_image\n",
    "        )\n",
    "        train_y.append(\n",
    "            category\n",
    "        )\n",
    "    for ii in test_indx:\n",
    "        image_data = cv2.imread(\n",
    "                os.path.join(path, images[ii])\n",
    "            )\n",
    "        resized_image = cv2.resize(\n",
    "            image_data, \n",
    "            (IMG_SIZE, IMG_SIZE)\n",
    "        )\n",
    "        test_X.append(\n",
    "            resized_image\n",
    "        )\n",
    "        test_y.append(\n",
    "            category\n",
    "        )\n",
    "\n",
    "train_X = np.array(train_X).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "train_y = np.array(train_y)\n",
    "test_X = np.array(test_X).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='synn'\n",
    "default_transformer_class = NeuralClassificationTransformer\n",
    "\n",
    "network = keras.Sequential()\n",
    "network.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        input_shape=np.shape(train_X[0]),\n",
    "    )\n",
    ")\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(\n",
    "    layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(\n",
    "    layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(\n",
    "    layers.Conv2D(\n",
    "        filters=254,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "network.add(layers.Flatten())\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dense(2000, activation=\"relu\"))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dense(2000, activation=\"relu\"))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dense(units=NUM_CLASS_PER_TASK, activation=\"softmax\")) \n",
    "\n",
    "default_transformer_kwargs = {\n",
    "    \"network\": network,\n",
    "    \"euclidean_layer_idx\": -2,\n",
    "    \"loss\": \"categorical_crossentropy\",\n",
    "    \"optimizer\": Adam(3e-4),\n",
    "    \"fit_kwargs\": {\n",
    "        \"epochs\": 200,\n",
    "        \"callbacks\": [EarlyStopping(patience=5, monitor=\"val_loss\")],\n",
    "        \"verbose\": False,\n",
    "        \"validation_split\": 0.33,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "}\n",
    "default_voter_class = KNNClassificationVoter\n",
    "default_voter_kwargs = {\"k\": int(np.log2(500))}\n",
    "default_decider_class = SimpleArgmaxAverage\n",
    "\n",
    "progressive_learner = ProgressiveLearner(\n",
    "        default_transformer_class=default_transformer_class,\n",
    "        default_transformer_kwargs=default_transformer_kwargs,\n",
    "        default_voter_class=default_voter_class,\n",
    "        default_voter_kwargs=default_voter_kwargs,\n",
    "        default_decider_class=default_decider_class,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 14:51:56.067436: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-28 14:51:58.052678: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/38 [..............................] - ETA: 7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 14:52:05.583709: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 8ms/step\n",
      "38/38 [==============================] - 0s 8ms/step\n",
      "13/13 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<proglearn.progressive_learner.ProgressiveLearner at 0x2d788c940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progressive_learner.add_task(\n",
    "            X=train_X,\n",
    "            y=train_y,\n",
    "            task_id=0,\n",
    "            num_transformers=1 if model == \"synn\" else ntrees,\n",
    "            transformer_voter_decider_split=[0.67, 0.33, 0],\n",
    "            decider_kwargs={\"classes\": np.unique(train_y)},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 1s 4ms/step\n",
      "108/108 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22267323861988983"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(progressive_learner.predict(\n",
    "            X=test_X, transformer_ids=[0], task_id=0\n",
    "        ) == test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(task=0):\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    test_X = []\n",
    "    test_y = []\n",
    "    \n",
    "    categories_to_consider = range(task*NUM_CLASS_PER_TASK,(task+1)*NUM_CLASS_PER_TASK)\n",
    "    for category in categories_to_consider:\n",
    "        path = os.path.join(TRAIN_DATADIR, str(category))\n",
    "\n",
    "        images = os.listdir(path)\n",
    "        total_images = len(images)\n",
    "        train_indx = sample(range(total_images), SAMPLE_PER_CLASS)\n",
    "        test_indx = np.delete(range(total_images), train_indx)\n",
    "        for ii in train_indx:\n",
    "            image_data = cv2.imread(\n",
    "                    os.path.join(path, images[ii])\n",
    "                )\n",
    "            resized_image = cv2.resize(\n",
    "                image_data, \n",
    "                (IMG_SIZE, IMG_SIZE)\n",
    "            )\n",
    "            train_X.append(\n",
    "                resized_image\n",
    "            )\n",
    "            train_y.append(\n",
    "                category\n",
    "            )\n",
    "        for ii in test_indx:\n",
    "            image_data = cv2.imread(\n",
    "                    os.path.join(path, images[ii])\n",
    "                )\n",
    "            resized_image = cv2.resize(\n",
    "                image_data, \n",
    "                (IMG_SIZE, IMG_SIZE)\n",
    "            )\n",
    "            test_X.append(\n",
    "                resized_image\n",
    "            )\n",
    "            test_y.append(\n",
    "                category\n",
    "            )\n",
    "\n",
    "    train_X = np.array(train_X).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "    train_y = np.array(train_y)\n",
    "    test_X = np.array(test_X).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "    test_y = np.array(test_y)\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model='synf', ntrees=10, rep=1, budget=40):\n",
    "    num_tasks = 50\n",
    "    tasks = []\n",
    "    base_tasks = []\n",
    "    accuracies_across_tasks = []\n",
    "    singletask_accuracy = []\n",
    "    df_multitask = pd.DataFrame()\n",
    "    df_singletask = pd.DataFrame()\n",
    "    transformers_to_consider = []\n",
    "    \n",
    "    if model == \"synn\":\n",
    "\n",
    "        clear_session()  # clear GPU memory before each run, to avoid OOM error\n",
    "\n",
    "        default_transformer_class = NeuralClassificationTransformer\n",
    "\n",
    "        network = keras.Sequential()\n",
    "        network.add(\n",
    "            layers.Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"relu\",\n",
    "                input_shape=(IMG_SIZE,IMG_SIZE,3),\n",
    "            )\n",
    "        )\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(\n",
    "            layers.Conv2D(\n",
    "                filters=32,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(\n",
    "            layers.Conv2D(\n",
    "                filters=64,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(\n",
    "            layers.Conv2D(\n",
    "                filters=128,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(\n",
    "            layers.Conv2D(\n",
    "                filters=254,\n",
    "                kernel_size=(3, 3),\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        network.add(layers.Flatten())\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation=\"relu\"))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(2000, activation=\"relu\"))\n",
    "        network.add(layers.BatchNormalization())\n",
    "        network.add(layers.Dense(units=NUM_CLASS_PER_TASK, activation=\"softmax\"))  # units=10\n",
    "\n",
    "        default_transformer_kwargs = {\n",
    "            \"network\": network,\n",
    "            \"euclidean_layer_idx\": -2,\n",
    "            \"loss\": \"categorical_crossentropy\",\n",
    "            \"optimizer\": Adam(3e-4),\n",
    "            \"fit_kwargs\": {\n",
    "                \"epochs\": 100,\n",
    "                \"callbacks\": [EarlyStopping(patience=5, monitor=\"val_loss\")],\n",
    "                \"verbose\": False,\n",
    "                \"validation_split\": 0.33,\n",
    "                \"batch_size\": 32,\n",
    "            },\n",
    "        }\n",
    "        default_voter_class = KNNClassificationVoter\n",
    "        default_voter_kwargs = {\"k\": int(np.log2(500))}\n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "\n",
    "    elif model == \"synf\":\n",
    "\n",
    "        default_transformer_class = TreeClassificationTransformer\n",
    "        default_transformer_kwargs = {\"kwargs\": {\"max_depth\": 30}}\n",
    "        default_voter_class = TreeClassificationVoter\n",
    "        default_voter_kwargs = {}\n",
    "        default_decider_class = SimpleArgmaxAverage\n",
    "\n",
    "    progressive_learner = ProgressiveLearner(\n",
    "        default_transformer_class=default_transformer_class,\n",
    "        default_transformer_kwargs=default_transformer_kwargs,\n",
    "        default_voter_class=default_voter_class,\n",
    "        default_voter_kwargs=default_voter_kwargs,\n",
    "        default_decider_class=default_decider_class,\n",
    "    )\n",
    "    \n",
    "    test_x_task = []\n",
    "    test_y_task = []\n",
    "    for task in range(num_tasks):\n",
    "        print(\"doing task \", task)\n",
    "        \n",
    "        if task > budget-1:\n",
    "            transformers_to_consider.pop(0)\n",
    "            \n",
    "        transformers_to_consider.append(task)\n",
    "        train_x, train_y, test_x, test_y = get_data(task)\n",
    "        train_x = train_x.reshape(-1, 3*IMG_SIZE*IMG_SIZE)\n",
    "        test_x = test_x.reshape(-1, 3*IMG_SIZE*IMG_SIZE)\n",
    "        \n",
    "        test_x_task.append(\n",
    "            test_x\n",
    "        )\n",
    "        test_y_task.append(\n",
    "            test_y\n",
    "        )\n",
    "        progressive_learner.add_task(\n",
    "            X=train_x,\n",
    "            y=train_y,\n",
    "            task_id=task,\n",
    "            num_transformers=1 if model == \"synn\" else ntrees,\n",
    "            transformer_voter_decider_split=[0.67, 0.33, 0],\n",
    "            decider_kwargs={\"classes\": np.unique(train_y)},\n",
    "        )\n",
    "\n",
    "        singletask_prediction = progressive_learner.predict(\n",
    "            X=test_x, transformer_ids=[task], task_id=task\n",
    "        )\n",
    "        singletask_accuracy.append(\n",
    "            np.mean(singletask_prediction==test_y)\n",
    "        )\n",
    "        print('accuracy ',np.mean(singletask_prediction==test_y))\n",
    "        for ii in range(task+1):\n",
    "            multitask_prediction = progressive_learner.predict(\n",
    "                X=test_x_task[ii], transformer_ids=transformers_to_consider, task_id=ii\n",
    "            )\n",
    "            acc = np.mean(multitask_prediction==test_y_task[ii])\n",
    "            print('task ',ii,' accuracy ', acc)\n",
    "            base_tasks.append(task+1)\n",
    "            tasks.append(ii+1)\n",
    "            accuracies_across_tasks.append(\n",
    "                np.mean(multitask_prediction == test_y_task[ii])\n",
    "            )\n",
    "\n",
    "    df_multitask['task'] = tasks\n",
    "    df_multitask['base_task'] = base_tasks\n",
    "    df_multitask['accuracy'] = accuracies_across_tasks\n",
    "\n",
    "    df_singletask['task'] = list(range(1,num_tasks+1))\n",
    "    df_singletask['accuracy'] = singletask_accuracy\n",
    "\n",
    "    summary = (df_multitask, df_singletask)\n",
    "\n",
    "    with open('results/'+model+'_'+str(rep)+'.pickle', 'wb') as f:\n",
    "        pickle.dump(summary, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
